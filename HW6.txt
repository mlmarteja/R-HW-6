#dataset
head(mtcars)
print(mtcars)
mtcars


##v engine = 0
##straight engine =1
##new column
mtcars$vs2 <-ifelse(mtcars$vs == 0,"V engine","straight engine")
mtcars
print(mtcars)
head(mtcars)


############data testing##############
indexes <- sample(2,nrow(mtcars),replace = T,prob = c(0.2,0.8))
indexes

training <- mtcars[indexes == 1,]
test <- mtcars[indexes == 2,]
training
test

###########naive payes################
install.packages("naivebayes")
library(naivebayes)

model<- naive_bayes(vs2~.,data=training,usekernel = T)
plot(model)

p <- predict(model,test)
p
table(p,test$vs2)



#######logistic regression##########
###a classification algorithm that calculates 
###the probability of fitting into one category
###will be a logistic curve

glm.fit<- glm(as.factor(vs2) ~ cyl+ disp + hp+drat +wt + qsec + vs + am + gear+carb+mpg,data = training, family = binomial)
summary(glm.fit)

glm.probs <- predict(glm.fit, test, type = "response")
table(glm.probs,test$vs2)

glm.pred<- ifelse(glm.probs > 0.5, "V Engine","Straight Engine")
attach(test)
table(glm.pred,vs2)


#######plot#######################
##import ggplot2
install.packages("ggplot2")
library(ggplot2)
install.packages("psych")
library(psych)
install.packages("dplyr")
library(dplyr)

###logistic regressions
ggplot(mtcars, aes(x=mpg, y=vs)) + 
  geom_point(alpha=.5, size=3) +
  stat_smooth(method="glm", se=FALSE, method.args = list(family=binomial))+
  ggtitle("Logistic Regression: mpy vs. vs2")


####naive bayes####
data <-mtcars
data$vs2 <- as.factor(data$vs2)
data$mpg <- as.factor(data$mpg)

# Visualization
pairs.panels(data[-1])

bp <- ggplot(data,aes(vs2,mpg, fill = vs2)) + 
  geom_boxplot() +
  ggtitle("Naive Bayes: mpg vs. vs2")+
  scale_y_continuous(limits = c(0, max(mtcars$mpg)))
bp

dp <- ggplot(data,aes(mpg, fill = vs2)) + 
  geom_density(alpha=.5) +
  ggtitle("Naive Bayes: mpg vs. vs2")
dp
